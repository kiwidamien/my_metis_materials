{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "\n",
    "Let's say we made a minor modification to a website, or to an ad. For example, we might have an email campaign with two slightly different headings\n",
    "\n",
    "|Variation | Text | Clicks | Impressions |\n",
    "|---|:---:|---|---|\n",
    "|A|\t\"Great savings inside\" |127 | 5734 |\n",
    "|B|\t\"Save up to 10% on your next order\" |174\t| 5851 |\n",
    "\n",
    "Here \"Impression\" means someone saw the variation, and \"click\" means they actually opened it and read it. \n",
    "\n",
    "How effective were the two campaigns? For the two campaigns, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022148587373561214\n",
      "0.029738506238249873\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "variation = namedtuple('variation', 'clicks impressions')\n",
    "\n",
    "A = variation(clicks=127, impressions=5734)\n",
    "B = variation(clicks=174, impressions=5851)\n",
    "\n",
    "for v in [A,B]:\n",
    "    rate = v.clicks/v.impressions\n",
    "    print(rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that campaign B (\"Save up to 10% on your next order\") was more effective at getting people to open the ad. Is this difference significant? To answer this, we can ask the following question:\n",
    "\n",
    "* If the ads were equally effective, what is the chance that variation B would beat variation A by this much?\n",
    "\n",
    "There are three different approaches we can take to this:\n",
    "1. Direct simulation\n",
    "2. A normal distribution test\n",
    "3. A $\\chi^2$ (chi-square) analysis\n",
    "\n",
    "We will investigate the first and second approaches here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance by simulation (post-hoc)\n",
    "\n",
    "1. Assume that both variations perform the same. Find the common conversion rate p\n",
    "2. Calculate if 5734 people come to variation A, what fraction $p_A$ of them convert (random sample with probability p)\n",
    "3. Calculate if 5851 people come to variation B, what fraction $p_B$ of them convert (random sample with probability p)\n",
    "4. Calculate the difference $|p_B - p_A|$. \n",
    "5. Repeat the steps 2 -- 4 a large number of times. Count the fraction of times that $|p_B - p_A|$ is bigger than the observed value (0.02974-0.02215 = 0.00759)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0101"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def draw_clicks_from_n_samples(impressions, prob_click, num_trials):\n",
    "    return np.random.binomial(impressions,prob_click,size=(num_trials,))\n",
    "\n",
    "def draw_p_sample_from_n_samples(impressions, prob_click, num_trials):\n",
    "    return draw_clicks_from_n_samples(impressions,prob_click,num_trials)/impressions\n",
    "\n",
    "num_trials = 10000\n",
    "imp_A, imp_B = 5734, 5851\n",
    "click_A, click_B = 127, 174\n",
    "\n",
    "difference_in_rate = abs(click_A/imp_A - click_B/imp_B)\n",
    "\n",
    "# What was the total conversion rate?\n",
    "p = (click_A + click_B)/(imp_A + imp_B)\n",
    "\n",
    "p_A_array = draw_p_sample_from_n_samples(imp_A, p, num_trials)\n",
    "p_B_array = draw_p_sample_from_n_samples(imp_B, p, num_trials)\n",
    "\n",
    "result_by_chance = (abs(p_B_array - p_A_array) > difference_in_rate)\n",
    "sum(result_by_chance)/len(result_by_chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value_from_simulation(num_trials, variation_A, variation_B):\n",
    "    imp_A, imp_B = variation_A.impressions, variation_B.impressions\n",
    "    click_A, click_B = variation_A.clicks, variation_B.clicks\n",
    "    \n",
    "    p = (click_A + click_B)/(imp_A + imp_B)\n",
    "    \n",
    "    difference_in_rate = abs(click_A/imp_A - click_B/imp_B)\n",
    "    \n",
    "    p_A_array = draw_p_sample_from_n_samples(imp_A, p, num_trials)\n",
    "    p_B_array = draw_p_sample_from_n_samples(imp_B, p, num_trials)\n",
    "    result_by_chance = (abs(p_B_array - p_A_array) > difference_in_rate)\n",
    "    return sum(result_by_chance)/len(result_by_chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0112"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_p_value_from_simulation(10000, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by normal distribution (post-hoc)\n",
    "\n",
    "Some notation:\n",
    "- the _sample proportion_ $p_A$ is what we measure when we do the experiment\n",
    "- the _population proportion_ $\\pi_A$ is the actual value of the mean (if we did an infinite number of samples)\n",
    "- If we have a sample size of $N$ (and $N \\gg 30$, as well as a couple of other technical points) then the proportion $p_A$ should be normally distributed with a mean of $\\pi_A$ and a variance of $\\pi_A(1-\\pi_A)/N\n",
    "\n",
    "We are looking for the difference between $\\pi_A$ and $\\pi_B$. The null hypothesis is that these rates are the _same_. The variance in the difference of the rates is \n",
    "$$\\sigma^2_{\\text{diff}} = \\frac{\\pi_A(1-\\pi_A)}{N_A} + \\frac{\\pi_B(1-\\pi_B)}{N_B}$$\n",
    "\n",
    "We want to how likely it is that $|p_A - p_B|$ is bigger than the value actually found in the experiment, if we assume $\\pi_A = \\pi_B$. Our $z$-score is\n",
    "$$|z| = \\frac{|p_A - p_B|}{\\sqrt{\\frac{p_A(1-p_A)}{N_A} + \\frac{p_B(1-p_B)}{N_B}}}$$\n",
    "\n",
    "We can use the CDF to find the probability that we get a $z$ score greater in magnitude than the one found. \n",
    "![normal distribution](image/normal.png)\n",
    "We want to find the total shaded area. The area to the left of $b$ is \n",
    "$$P(z > b) = 1 - cdf(b)$$\n",
    "Since the area less than $-b$ is the same (symmetry of the normal distribution with mean 0), we know the $P$ value is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{p-value} = P(|z| > b) = P(z<-b) + P(z>b) = 2 P(z > b) = 2(1 - cdf(b))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010242813991217181"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "p_A, p_B = A.clicks/A.impressions, B.clicks/B.impressions\n",
    "p = (A.clicks + B.clicks)/(A.impressions + B.impressions)\n",
    "variance = p*(1-p)/A.impressions + p*(1-p)/B.impressions # null hypothesis - only one p\n",
    "\n",
    "abs_z = abs(p_A - p_B)/np.sqrt(variance)\n",
    "\n",
    "p_value = 2*(1-norm.cdf(abs_z))\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to encapsulate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value_analytic(variationA, variationB):\n",
    "    p_A, p_B = (variationA.clicks/variationA.impressions, \n",
    "                variationB.clicks/variationB.impressions)\n",
    "    p = (A.clicks + B.clicks)/(A.impressions + B.impressions)\n",
    "    variance = p*(1-p)/variationA.impressions + p*(1-p)/variationB.impressions\n",
    "    \n",
    "    abs_z = abs(p_A - p_B)/np.sqrt(variance)\n",
    "    \n",
    "    p_value = 2*(1-norm.cdf(abs_z))\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010242813991217181"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_p_value_analytic(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc vs apriori\n",
    "\n",
    "The tests described so far are how we find the $p$-value _after the experiment has concluded_. The $p$-value tells us whether the result is statistically significantly different from chance (specifically, it tells us the chance of a result this large happening if we assume there is no difference in the variations). \n",
    "\n",
    "There are lots of misconceptions about $p$-values. This is a good time to address one of them: the idea that the lower the $p$-value, the better one variation is than the other. This is _mostly false_. If two variations are different (and let's face it, there is always __some__ difference), then for a large enough sample size $N$, you can find a tiny $p$-value. At a **fixed** sample size $N$, it is true that bigger differences in proportion lead to smaller $p$-values (which is the origin of this misconception).\n",
    "\n",
    "Why do we care about this? When designing an A/B test, one of the questions we should answer before collecting any data is\n",
    "> We have two variations (A and B). Before the experiment, we want to know how long we should run the experiment before drawing a conclusion. How long should that be?\n",
    "\n",
    "i.e. we don't know the size of the experiment $N$; that is what we are trying to determine. The longer we run the experiment, the more we are \"missing out\" on the opportunity costs of employing the \"better\" variation. We know we can detect the difference if we run the experiment long enough, but we should know \"is it worth it?\"\n",
    "\n",
    "To answer this, we need to know\n",
    "- what difference in proportion is worth measuring?\n",
    "- the expected rate of visitors at our site\n",
    "- the period of cycles within our buisness (often weekly or monthly)\n",
    "\n",
    "Then we can determine how long we should run the experiment in order to detect the difference.\n",
    "\n",
    "### How to do it: statistical power\n",
    "\n",
    "The $p$-value asks the question: if there is no difference, what is the probability that our experiment finds a result at least as big as the one we found? If we declare a cutoff on the $p$-value of 5%, we are claiming that we are willing to make a __Type I error__ 5% of the time. We calculate $p$-values after the experiment. The cutoff for the $p$-value is often denoted $\\alpha$.\n",
    "\n",
    "The _power_ $\\beta$ asks the question: if there __is__ a difference of size $\\Delta p$, what is the propability that our experiment (with the cutoff) finds it? In order to answer this question, we have to give the full experimental procedure. It is related to the chance of a __Type II error__, but isn't the same as it.\n",
    "\n",
    "Let's take an experiment with $\\alpha = 5\\%$ (i.e. we claim that we don't have evidence for the two variations being different if $p > 0.05$, and if $p<0.05$ we claim one a winner but acknowledge we would get this result 5% of the time if the variations are the same). Using knowledge of the normal distribution, the procedure is:\n",
    "1. Calculate the $z$-score under the null hypothesis:\n",
    "\\begin{equation*}\n",
    "|z_0| = \\frac{|p_A - p_B|}{\\sqrt{\\frac{p(1-p)}{N_A} + \\frac{p(1-p)}{N_b}}}\n",
    "\\end{equation*}\n",
    "Here $p$ is the pooled probability.\n",
    "2. If $|z_0| < 1.96$, then we claim no winner\n",
    "3. If $|z_0| \\geq 1.96$, we declare the winner to be the higher probability.\n",
    "\n",
    "#### The math background\n",
    "\n",
    "Let's analyize this same experiment under the hypothesis that there is a difference $\\Delta p$, i.e. $\\pi_B - \\pi_A = \\Delta\\pi$. We want to know, __assuming there is a difference of size $\\Delta \\pi$__, what is the chance that the experiment above gets $|z_0| > 1.96$? \n",
    "\n",
    "If $z_0= 1.96$, and assuming $p_B$ is the variation that did better, this tells us \n",
    "$$p_B - p_A = 1.96 \\sqrt{\\frac{p(1-p)}{N_A} + \\frac{p(1-p)}{N_B}}$$\n",
    "This is the \"critical value\" -- differences bigger than this will lead to acceptance, while difference smaller than this lead to rejection. What is the $z$ score of this difference under the hypothesis that there is a difference?\n",
    "\\begin{equation*}\n",
    "|z_2| = \\frac{(\\pi_B - \\pi_A) - (p_B - p_A)}{\\sigma_{\\Delta p}} = \\frac{\\Delta \\pi - 1.96 \\sqrt{\\frac{p(1-p)}{N_A} + \\frac{p(1-p)}{N_B}}}{\\sqrt{\\frac{p_A(1-p_A)}{N_A} + \\frac{p_B(1-p_B)}{N_B}}}\n",
    "\\end{equation*}\n",
    "We can simplify this a lot if we assume that $N_A = N_B$:\n",
    "\\begin{equation*}\n",
    "|z_2| =  \\frac{\\Delta \\pi - 1.96 \\sqrt{\\frac{2p(1-p)}{N}}}{\\sqrt{\\frac{p_A(1-p_A)+p_B(1-p_B)}{N}}} = \\frac{\\Delta \\pi \\sqrt{N} - 1.96 \\sqrt{2p(1-p)}}{\\sqrt{p_A(1-p_A)+p_B(1-p_B)}}\n",
    "\\end{equation*}\n",
    "We now solve for $N$, the number needed for each variation:\n",
    "\n",
    "\\begin{equation*}\n",
    "N = \\frac{(|z_2|\\sqrt{p_A(1-p_A) + p_B(1-p_B)} + 1.96\\sqrt{2p(1-p)})^2}{(\\Delta \\pi)^2}\n",
    "\\end{equation*}\n",
    "\n",
    "#### The procedure\n",
    "\n",
    "So what does all that mean? Let's walk through an example. Let's say that we have an email campaign with headline \"Great Savings inside\" that gets about 2% click-throughs. We want to run an experiment with an alternative heading (\"Save up to 10% on your next order\") at a confidence level of $\\alpha = 5\\%$. We want to be able to detect differences of $1\\%$ at least $80\\%$ of the time.\n",
    "\n",
    "Our question: How many emails $N$ of each variation do we need to run the experiment?\n",
    "\n",
    "Our answer:\n",
    "1. Figure out $z_2$, so that $P(z > z_2) = 0.8$. In this case, we use `z2 = norm.ppf(1-0.8) = -0.8416`, meaning that you have a z-score higher than $-0.8416$ in 80% of trials.\n",
    "2. Get parameters: \n",
    "  We have $p_A = 2\\%$ (historical), and $p_B = 3\\%$ (detect a 1\\% improvement). We have $\\Delta \\pi = 1\\%$ as well. The pooled conversion rate is $p = (p_A + p_B)/2 = 2.5\\%$. It is a simple average because we are assuming equal numbers of people to our variations.\n",
    "3. Do the calcuation, in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3825.1497221026216"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha, beta = 0.05, 0.8\n",
    "p_A=0.02\n",
    "DeltaPi = 0.01\n",
    "\n",
    "p_B = p_A + DeltaPi\n",
    "p = 0.5*(p_A + p_B)\n",
    "\n",
    "z2 = abs(norm.ppf(1-beta))\n",
    "z_crit = norm.ppf(1-alpha/2)  # this is 1.96\n",
    "\n",
    "root_N = (z2*np.sqrt(p_A*(1-p_A) + p_B*(1-p_B)) + z_crit*np.sqrt(2*p*(1-p))) / DeltaPi\n",
    "N = root_N**2\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. you should plan on having 3826 people in variationA and 3826 people in variationB. If we have fewer than this number of people then we probably will conclude that there is no difference. Note that larger differences (larger $\\Delta\\pi$) are easier to detect than smaller differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_function(p_A_base, DeltaPi, alpha=0.05, beta=0.8):\n",
    "    p_B = p_A + DeltaPi\n",
    "    p = 0.5*(p_A_base + p_B)\n",
    "    \n",
    "    z2 = abs(norm.ppf(1-beta))\n",
    "    z_crit = norm.ppf(1-alpha/2)\n",
    "    root_N = (z2*np.sqrt(p_A*(1-p_A) + p_B*(1-p_B)) + z_crit*np.sqrt(2*p*(1-p)))/ DeltaPi\n",
    "    return int(np.ceil(root_N**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3826"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same idea as hypothetical above, wrapped in a function\n",
    "power_function(0.02, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why $(1-\\beta)$ is not (quite) the Type II error\n",
    "\n",
    "You can see a lot of statements that if $\\beta$ is the statistical power, then $1-\\beta$ is the type II error rate. This isn't quite right. The way we defined it, $1-\\beta$ is the probability of making a type II error _if we know the difference in proportion is exactly our threshold of caring_. If it is likely that the difference in proportion is higher than our threshold of caring, then $1-\\beta$ is an overestimate of the type II error (because the difference is easier to find than we cared about). Similiarly, if the difference is smaller than expected, $1-\\beta$ is an underestimate of the type II error.\n",
    "\n",
    "The actual probability of a type II error occuring depends on the _prior_ distribution of probability differences. We haven't specified a prior here, so we cannot calculate the type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical issues with A/B testing\n",
    "\n",
    "Let's continue our example. If we have a \"bank\" of customers, we could fire off 3826 emails of variation A, 3826 emails of variation B, and then if B performed better, we could email the rest of our customers variation B. If A did better, or there was no clear winner, we would fall back to the default of A.\n",
    "\n",
    "Let's say that we were interested in only targeting new signups, and that our site had 3000 people join a week. Then how long would we run our test?\n",
    "\n",
    "A naive estimate would be that we need a total of 3826 * 2 = 7652 people. We have 3000 / 7 = 428 people per day. So maybe we should run the experiment for 7652 / 428 = 17 days?\n",
    "\n",
    "This is fine, provided the type of people that visit our site on Monday are the same as the people that visit on Saturday. Often, websites have different volumes of traffic depending on day of the week. Employed people have different representation during the week than after it (over- or under-representation depends on the site: EW.com has a different profile than LinkedIn)\n",
    "\n",
    "If there is a weekly cycle, you are better to round up to the nearest whole week. In this case: 7652 / 3000 = 2.55, so I would run for three weeks before drawing a conclusion. If I was looking at high cost purchases, I might see monthly patterns in my sales (as most people are paid monthly). If I did see significant monthly variation, I would push to extend the experiment to run for a month.\n",
    "\n",
    "You should also consider (at design time) splitting your customers into clusters. Just because variationA outperforms variationB over the entire population, you can find that variationB outperfoms variationA on some subset (e.g. people on the west coast, people that log on after 5 pm, et cetra). You can choose to show different variations for different clusters. If you do this, you should know that the power calculations for sample size are the number of people that need to see each variation _per cluster_, rather than overall.\n",
    "\n",
    "We have phrased A/B testing in terms of clicking through an email. Generally there are two types of A/B tests I have seen:\n",
    "1. Engagement: do people who see variation A use the site more than people who see variation B? We are interested in optimizing the return rate.\n",
    "2. Call-to-action: Each view of a variation asks a user to do something (e.g. open the email). We are interested in optimizing the success rate, which is between 0 and 1.\n",
    "\n",
    "Our methods have all been based around \"call-to-action\" (see the email, then open it). Sometimes it is less clear cut, especially for \"Engagement\" campaigns. If you change the Duolingo Owl graphics to see if you can entice people back to learn a language more, are you going to count the number of visits that user has to the site for the next day as correlated with the campaign? The next week? The next month? Determining the answer to these questions is often a mix of looking at historical records of your users, and experimentation.\n",
    "\n",
    "Another common use-case is looking at user-flow through a sign-up process. When do the users leave? If you show them a different process, do more of them sign up? Are you optimizing for total number of signups, or are you interested in only particular demographics. To avoid multiple testing, you should plan you experiment (including the metrics) **before** running the experiment.\n",
    "\n",
    "A case that has less to do with web-development: you can also A/B test your pricing and discount structures. Uber and Lyft do this when experimenting with surge pricing and incentives for drivers. Here you have to be particularly careful, because the experiments can interfere with each other. A great example of this is on Lyft's blog:\n",
    "\n",
    "https://eng.lyft.com/experimentation-in-a-ridesharing-marketplace-b39db027a66e\n",
    "\n",
    "The experiment here was to compare a passengers that didn't get charged a surge with those that do. In an experiment where there is only one car nearby to pickup, the person with the cheaper fare will be more likely to request a ride than the person with the more expensive fare. This is what Lyft want to measure. But the effect is compounded by \"blocking\": whoever takes the car first stops the other person from taking the same car. In a naive A/B test or simulation where you randomly assign people to the variations, the people with the discount are expected to take $3\\times$ more rides compared to those that don't. If you design the experiment differently to eliminate \"blocking\" (e.g. discount surges for everyone at randomly selected times) the actual effect is closer to a $1.33$ multiplier -- a huge difference!\n",
    "\n",
    "\n",
    "You should aslo communicate to the devs on your team that, where possible, the same user should see the same variation each time. If Amazon is running an A/B test with green \"buy\" buttons, they should make sure that I see a green \"buy\" button until the experiment is over. Randomly seeing different elements can make people skeptical about using a site.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What not to do: early stopping\n",
    "\n",
    "It might be tempting to skip all the power calculations and just work \"until you have reached a p-value of 0.05\" and then take the winner.\n",
    "\n",
    "**This is bad**\n",
    "\n",
    "The p-value assumes that you do the check _once_. As a rough guide, if you do $N$ checks, if there is no difference, each time you have a probability of $\\alpha$ of (falsely) claiming a winner. The probability of (correctly) claiming at no one winners in N trials is roughly\n",
    "$$(1-\\alpha)^N$$\n",
    "I say roughly, because the trails are not independent of each other. But assuming this formula is correct, here is a table showing the effect of multiple evaluations for $\\alpha = 0.05$\n",
    "\n",
    "| Number of evaluations | Prob of correctly saying no difference | Prob of concluding difference |\n",
    "|---:|---:|---:|\n",
    "| 1 | 0.95 | 0.05 |\n",
    "| 2 | 0.90 | 0.10 |\n",
    "| 5 | 0.77 | 0.23 |\n",
    "|10 | 0.59 | 0.41 |\n",
    "\n",
    "Let's run a simulation, where at the end of the day we do a significance test until we reach the end. We will assume 200 visits a day (100 for each variation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_day(pA, pB, num_visits = 100):\n",
    "    cA = draw_clicks_from_n_samples(num_visits, pA, 1)[0]\n",
    "    cB = draw_clicks_from_n_samples(num_visits, pB, 1)[0]\n",
    "    return (variation(clicks=cA, impressions=num_visits), \n",
    "            variation(clicks=cB, impressions=num_visits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_variation(var_before, var_new):\n",
    "    return variation(clicks=var_before.clicks + var_new.clicks,\n",
    "                     impressions = var_before.impressions + var_new.impressions)\n",
    "\n",
    "def get_prop_from_variation(var):\n",
    "    return var.clicks / var.impressions\n",
    "\n",
    "def run_test_with_peeking(n_days, pA, pB, alpha = 0.05):\n",
    "    cumulative_var_A = variation(clicks=0, impressions=0)\n",
    "    cumulative_var_B = variation(clicks=0, impressions=0)\n",
    "    for day in range(n_days):\n",
    "        varA, varB = do_day(pA, pB)\n",
    "        cumulative_var_A = cumulative_variation(cumulative_var_A, varA)\n",
    "        cumulative_var_B = cumulative_variation(cumulative_var_B, varB)\n",
    "        p_value = get_p_value_analytic(cumulative_var_A, cumulative_var_B)\n",
    "        if p_value < alpha:\n",
    "            if get_prop_from_variation(cumulative_var_A) > get_prop_from_variation(cumulative_var_B):\n",
    "                winner = 'A'\n",
    "            else:\n",
    "                winner = 'B'\n",
    "            return {\n",
    "                'day': day,\n",
    "                'winner': winner,\n",
    "                'p': p_value\n",
    "            }\n",
    "    return {\n",
    "        'day': day,\n",
    "        'winner': None,\n",
    "        'p': p_value\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 9, 'winner': 'B', 'p': 0.04908410065122215}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run a single time with pA = pB = 5% over 30 days\n",
    "run_test_with_peeking(30, 0.05,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate 1000 experiments. How often are they called?\n",
    "all_exp = [run_test_with_peeking(30, 0.05, 0.05) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With peeking and alpha=0.05, we had a type 1 error rate of 0.623\n"
     ]
    }
   ],
   "source": [
    "# How many had the correct answer, None, as the winner?\n",
    "num_type_1_errors = len([e for e in all_exp if e['winner'] is not None])\n",
    "frac_type_1_errors = num_type_1_errors/len(all_exp)\n",
    "print(\"With peeking and alpha=0.05, we had a type 1 error rate of {}\".format(frac_type_1_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# How long did it take to reach the incorrect conclusion?\n",
    "num_days = [e['day'] for e in all_exp if e['winner'] is not None]\n",
    "plt.hist(num_days, bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative to early stopping: multi-armed bandit\n",
    "\n",
    "The multi-armed bandit approach to the same problem is modelling off the following: each variation is like a slot machine. You have a certain number of customers to \"spend\", and you might get a reward if they win. You have to decide how to \"spend your customers\" (i.e. how to split them amongst the different variations)\n",
    "\n",
    "The basic idea is simple:\n",
    "1. Play a \"few\" rounds, and see how well each variation does (exploratory phase)\n",
    "2. When a user arrives, assign her randomly to a variation. Variations that are doing better (i.e. have higher conversion rates) have a higher probability of being picked.  Record what the new user does (success vs fail). Update the probabilities to reflect current conversion rates. (exploitation phase).\n",
    "\n",
    "The idea is that variations that do well will be selected more often, and variations that do poorly will \"die out\". There are a couple of different ways of implementing step 2 (mapping from success rates to chance of picking a variation).\n",
    "\n",
    "Two common methods are __epsilon greedy__ and __thompson sampling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon greedy (technically epsilon-first greedy)\n",
    "\n",
    "Basic setup:\n",
    "\n",
    "You will need to pick $N$ (the number of users you use to evaluate the variations initially), and $\\epsilon$ (the probability of exploring).\n",
    "\n",
    "1. Use the first N users (you choose N) to determine the success rate of each variation\n",
    "2. For each user, pick a number between 0 and 1. If the number is less than $\\epsilon$, randomly choose a variation. Otherwise, just send the user to the current best variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSuccess(prob):\n",
    "    return np.random.rand() < prob\n",
    "\n",
    "def epsilon_greedy_sim(N0, epsilon, actual_probs, num_trials):\n",
    "    \"\"\"\n",
    "    N0: integer, number of users to send to each variation\n",
    "    epsilon: float between 0 and 1. Probability of exploring new variation\n",
    "    actual_probs: a list of probabilities of success for each variation\n",
    "    num_trials: num of users total to use after the initialization\n",
    "    \n",
    "    returns a list of success rates found by the simulation\n",
    "    \"\"\"\n",
    "    current = [variation(clicks=sum([isSuccess(p) for _ in range(N0)]), impressions=N0)\n",
    "                     for p in actual_probs]\n",
    "    for _ in range(num_trials):\n",
    "        if isSuccess(epsilon):\n",
    "            # randomly pick a variation\n",
    "            var_to_do = np.random.randint(0,len(actual_probs) - 1)\n",
    "        else:\n",
    "            var_to_do = np.argmax(np.array([get_prop_from_variation(v) for v in current]))\n",
    "        current[var_to_do] = cumulative_variation(current[var_to_do], \n",
    "                                                  variation(clicks = isSuccess(actual_probs[var_to_do]), impressions=1))\n",
    "        \n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[variation(clicks=417, impressions=9577),\n",
       " variation(clicks=14, impressions=623),\n",
       " variation(clicks=3, impressions=100)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is an example using 3 variations with 4% conversion rate, 3% and 2% respectively\n",
    "np.random.seed(42)\n",
    "epsilon_greedy_sim(100, 0.1, [0.04, 0.03,0.02], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this identified mostly variation A, which is what we want. If the variations are close, a sub-par variation can take the lead early, and then keeps it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[variation(clicks=72, impressions=1775),\n",
       " variation(clicks=257, impressions=8425),\n",
       " variation(clicks=1, impressions=100)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.seed(60)\n",
    "epsilon_greedy_sim(100, 0.1, [0.04, 0.03,0.02], 10000) # burn a random seed\n",
    "epsilon_greedy_sim(100, 0.1, [0.04, 0.03,0.02], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ways to avoid this:\n",
    "- Increase $N0$: require more sampling initially, so it is harder for a bad variation to get a head start\n",
    "- Increase $\\epsilon$: Make more random pulls. This gives good variations more of a chance to catch up\n",
    "\n",
    "Note that both of these techniques require me to give up making the current best choice. The big difference is $\\epsilon$ is \"forever\" -- if I have $\\epsilon=0.15$ I am deciding that I am always experimenting with 15% of my data. \n",
    "\n",
    "If $\\epsilon$ is small, we cannot catch up from a suboptimal variation that started with a string of good luck.\n",
    "\n",
    "If two variations have similar conversion rates, then it is easier for the suboptimal ones to \"win\". The motivation of using the bandit algorithms is you don't \"waste\" as much time finding the best variation, and even if you pick a suboptimal variation it is likely to be a better suboptimal variation.\n",
    "\n",
    "#### Pitfall\n",
    "\n",
    "If you have weekly variation in your data, you don't want to make your initial data collection less than a week. If a traditional A/B test would work in that time frame, then you should do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson sampling\n",
    "\n",
    "Basic idea:\n",
    "1. Start every variation as having a success probability drawn from p_i ~ Beta(1,1) i.e. uniform\n",
    "2. When a new user comes in, draw a random probability of success from each Beta(S_i+1, F_i+1) distribution, where $S_i$ is the number of successes in variation $i$, and $F_i$ is the number of failures in variation $i$.\n",
    "3. Assign the user to whichever variation had the largest probability drawn in step 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000386523859661"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "beta.rvs(a = 1, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thompson_sim(actual_probs, num_trials):\n",
    "    \"\"\"\n",
    "    actual_probs: a list of probabilities of success for each variation\n",
    "    num_trials: num of users total to use after the initialization\n",
    "    \n",
    "    returns a list of variations about what happened in the experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    current = [(1,1) for _ in actual_probs]\n",
    "    for _ in range(num_trials):\n",
    "        prob_draw = np.array([beta.rvs(Alpha,Beta) for Alpha,Beta in current])\n",
    "        to_use = np.argmax(prob_draw)\n",
    "        trial_success = isSuccess(actual_probs[to_use])\n",
    "        current[to_use] = (current[to_use][0] + trial_success, current[to_use][1] + (1-trial_success))\n",
    "\n",
    "    return [variation(clicks=Alpha , impressions=Alpha + Beta) for Alpha, Beta in current]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[variation(clicks=362, impressions=9209),\n",
       " variation(clicks=7, impressions=385),\n",
       " variation(clicks=8, impressions=412)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "thompson_sim([0.04, 0.03,0.02], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and cons\n",
    "\n",
    "| Property | A/B testing (p-value)| MAB: Epsilon | MAB: Thompson |\n",
    "|---| --- | --- | --- |\n",
    "| Fixed time | Yes | No | No|\n",
    "| Continual costs | No (experiment ends) | Yes | Yes |\n",
    "| Parameters | $\\alpha$, power, time | $N_0$, $\\epsilon$, time| time |\n",
    "| Early Leaders bias results | No | Yes, strongly | Yes, but only mild |\n",
    "| Adaptive | No | Yes | Yes |\n",
    "| _Natural_ generalizes to many variations | No(\\*) | Yes | Yes |\n",
    "| Makes Type I guarantees | Yes | No | No |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
